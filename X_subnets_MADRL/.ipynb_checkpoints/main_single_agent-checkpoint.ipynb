{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f6485ef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# main.py\n",
    "import numpy as np\n",
    "import gym\n",
    "#from DDPG_trial import DDPG\n",
    "from collections import deque\n",
    "import pdb\n",
    "import torch\n",
    "from env import env\n",
    "from LSTM import LSTMModel  \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import pdb \n",
    "M, N, J = 10, 4, 3\n",
    "# Replay Buffer for Experience Replay\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, max_size=int(1e6)):\n",
    "        self.buffer = deque(maxlen=max_size)\n",
    "\n",
    "    def add(self, state, next_state, action, reward):\n",
    "        self.buffer.append((state, next_state, action, reward))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        batch = random.sample(self.buffer, batch_size)\n",
    "        state, next_state, action, reward = map(np.stack, zip(*batch))\n",
    "        return (\n",
    "            torch.FloatTensor(state),\n",
    "            torch.FloatTensor(next_state),\n",
    "            torch.FloatTensor(action),\n",
    "            torch.FloatTensor(reward).unsqueeze(1),\n",
    "            #torch.FloatTensor(done).unsqueeze(1),\n",
    "        )\n",
    "\n",
    "    def size(self):\n",
    "        return len(self.buffer)\n",
    "\n",
    "###########generate random discrete matrix for actions(should ideally come from the discretization procedure)\n",
    "\n",
    "\n",
    "def generate_matrix(N, J):\n",
    "    # Initialize an NxJ matrix filled with zeros\n",
    "    matrix = np.zeros((N, J), dtype=int)\n",
    "    \n",
    "    # Ensure each column has at least one 1\n",
    "    for j in range(J):\n",
    "        # Randomly select a row to place the 1 in column j\n",
    "        while True:\n",
    "            row_index = np.random.randint(N)\n",
    "            # Place the 1 only if that row currently contains no 1\n",
    "            if np.sum(matrix[row_index]) == 0:\n",
    "                matrix[row_index, j] = 1\n",
    "                break\n",
    "    \n",
    "    # Fill remaining rows with either a single 1 or all 0s\n",
    "    for i in range(N):\n",
    "        if np.sum(matrix[i]) == 0:  # If the row is all zeros\n",
    "            if np.random.rand() > 0.5:  # 50% chance to add a 1\n",
    "                col_index = np.random.randint(J)\n",
    "                matrix[i, col_index] = 1\n",
    "    \n",
    "    return matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e1991eb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sriniva3\\AppData\\Local\\Temp\\ipykernel_35280\\1832167845.py:16: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load('LSTM_state_dict.pth'))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#model = torch.load('LSTM.pth')\n",
    "# Initialize the model architecture\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Model parameters\n",
    "input_dim = 4\n",
    "hidden_dim = 128\n",
    "output_dim = 4\n",
    "num_layers = 1\n",
    "\n",
    "# Initialize the model, loss function and optimizer\n",
    "model = LSTMModel(input_dim, hidden_dim, output_dim, num_layers).to(device)\n",
    "#model = LSTMModel()  # Initialize your model with the required parameters\n",
    "\n",
    "# Load the saved state dictionary\n",
    "model.load_state_dict(torch.load('LSTM_state_dict.pth'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "42bdbba1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 1080/1080 [03:29<00:00,  5.16it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████| 9999/9999 [00:03<00:00, 3285.69it/s]\n"
     ]
    }
   ],
   "source": [
    "Ts = 10000\n",
    "environ = env(Ts = Ts)\n",
    "alltime_fast_fading_gains = environ.fast_fading_channel_coefficients()\n",
    "TxRxds = environ.compute_TxRX()\n",
    "alltime_PathGains = environ.large_scale_fading_channel_coefficients(TxRxds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c468a6b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lstm_pred_SINR(ts_start, ts_end):\n",
    "    history = 10\n",
    "    SINR_nxt_lstm = np.zeros((J,N))\n",
    "    for j in range(J):\n",
    "        inp = []\n",
    "        for ts in np.arange(ts_start, ts_end):\n",
    "            SINR =  environ.get_next_state(alltime_PathGains, alltime_fast_fading_gains, ts = ts)[0]\n",
    "            inp.append(SINR[:,j])\n",
    "        inp = np.array(inp).reshape(1,history,N)\n",
    "        SINR_nxt_lstm[j,:] = model(torch.Tensor(inp)).detach().numpy()[0]\n",
    "    SINR_nxt_lstm = np.transpose(SINR_nxt_lstm)\n",
    "    return SINR_nxt_lstm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "42f0d22b",
   "metadata": {},
   "outputs": [],
   "source": [
    "b_prev_actions =  generate_matrix(N, J)\n",
    "state_lstm = get_lstm_pred_SINR(ts_start = 0, ts_end = 10)\n",
    "state = np.stack((np.multiply(state_lstm, b_prev_actions),b_prev_actions), axis = 0)\n",
    "b_prsnt_actions =  generate_matrix(N, J)\n",
    "nxt_state_lstm = get_lstm_pred_SINR(ts_start = 1, ts_end = 11)\n",
    "next_state = np.stack((np.multiply(nxt_state_lstm, b_prsnt_actions),b_prsnt_actions), axis = 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bedab158",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f3fe94a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f83097c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "157d7864",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Actor Network\n",
    "class Actor(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim, max_action):\n",
    "        super(Actor, self).__init__()\n",
    "        self.fc1 = nn.Linear(state_dim, 256)\n",
    "        self.fc2 = nn.Linear(256, 128)\n",
    "        self.fc3 = nn.Linear(128,64)\n",
    "        self.fc4 = nn.Linear(64, action_dim)\n",
    "        self.max_action = max_action\n",
    "\n",
    "    def forward(self, state):\n",
    "        x = F.relu(self.fc1(state))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = torch.sigmoid(self.fc4(x))\n",
    "        return x * self.max_action\n",
    "\n",
    "# Critic Network\n",
    "class Critic(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        super(Critic, self).__init__()\n",
    "        self.fc1 = nn.Linear(state_dim + action_dim, 128)\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.fc3 = nn.Linear(64, 1)\n",
    "\n",
    "    def forward(self, state, action):\n",
    "        x = torch.cat([state, action], 1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        q_value = self.fc3(x)\n",
    "        #pdb.set_trace()\n",
    "        return q_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "de839f08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DDPG Agent\n",
    "class DDPG:\n",
    "    def __init__(self, state_dim, action_dim, max_action):\n",
    "        self.actor = Actor(state_dim, action_dim, max_action).to(torch.device(\"cpu\"))\n",
    "        self.actor_target = Actor(state_dim, action_dim, max_action).to(torch.device(\"cpu\"))\n",
    "        self.actor_target.load_state_dict(self.actor.state_dict())\n",
    "        self.actor_optimizer = optim.Adam(self.actor.parameters(), lr=1e-3)\n",
    "\n",
    "        self.critic = Critic(state_dim, action_dim).to(torch.device(\"cpu\"))\n",
    "        self.critic_target = Critic(state_dim, action_dim).to(torch.device(\"cpu\"))\n",
    "        self.critic_target.load_state_dict(self.critic.state_dict())\n",
    "        self.critic_optimizer = optim.Adam(self.critic.parameters(), lr=1e-3)\n",
    "\n",
    "        self.max_action = max_action\n",
    "        self.discount = 0.99\n",
    "        self.tau = 0.005\n",
    "\n",
    "    def select_action(self, state):\n",
    "        state = torch.FloatTensor(state.reshape(1, -1)).to(torch.device(\"cpu\"))\n",
    "        action = self.actor(state).cpu().data.numpy().flatten()\n",
    "        return action\n",
    "\n",
    "    def train(self, replay_buffer, t, batch_size=64):\n",
    "        # Sample a batch of transitions from the replay buffer\n",
    "        state, next_state, action, reward = replay_buffer.sample(batch_size)\n",
    "        next_state = next_state.view(batch_size,-1)\n",
    "        state = state.view(batch_size,-1)\n",
    "        action = action.view(batch_size,-1)\n",
    "        # Compute the target Q value\n",
    "        #pdb.set_trace()\n",
    "        next_action = self.actor_target(next_state)\n",
    "        target_q = self.critic_target(next_state, next_action)\n",
    "        #target_q1, target_q2 = self.critic_target(next_state, next_action)\n",
    "        #target_q = torch.min(target_q1, target_q2)\n",
    "        if t < 256:\n",
    "            not_done = 1.0\n",
    "        else:\n",
    "            not_done = 0.0\n",
    "        #pdb.set_trace()\n",
    "        target_q = reward + not_done * self.discount * target_q.detach()\n",
    "\n",
    "        # Get current Q estimate\n",
    "        current_q = self.critic(state, action)\n",
    "        #current_q1, current_q2 = self.critic(state, action)\n",
    "        #current_q = torch.min(current_q1, current_q2)\n",
    "        # Compute critic loss\n",
    "        critic_loss = F.mse_loss(current_q, target_q)\n",
    "\n",
    "        # Optimize the critic\n",
    "        self.critic_optimizer.zero_grad()\n",
    "        critic_loss.backward()\n",
    "        self.critic_optimizer.step()\n",
    "\n",
    "        # Compute actor loss\n",
    "        actor_loss = -self.critic(state, self.actor(state)).mean()\n",
    "\n",
    "        # Optimize the actor\n",
    "        self.actor_optimizer.zero_grad()\n",
    "        actor_loss.backward()\n",
    "        self.actor_optimizer.step()\n",
    "\n",
    "        # Update the target networks\n",
    "        for param, target_param in zip(self.critic.parameters(), self.critic_target.parameters()):\n",
    "            target_param.data.copy_(self.tau * param.data + (1 - self.tau) * target_param.data)\n",
    "\n",
    "        for param, target_param in zip(self.actor.parameters(), self.actor_target.parameters()):\n",
    "            target_param.data.copy_(self.tau * param.data + (1 - self.tau) * target_param.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "976c6f4a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29b9969b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df0c69c2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "092eb40e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "07dcbca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "T = 256\n",
    "############# get initial states  #############\n",
    "'''\n",
    "b_prev_actions =  generate_matrix(N, J)\n",
    "state_lstm = get_lstm_pred_SINR(ts_start = 0, ts_end = 10)\n",
    "state = np.stack((np.multiply(state_lstm, b_prev_actions),b_prev_actions), axis = 0)\n",
    "b_prsnt_actions =  generate_matrix(N, J)\n",
    "nxt_state_lstm = get_lstm_pred_SINR(ts_start = 1, ts_end = 11)\n",
    "next_state = np.stack((np.multiply(nxt_state_lstm, b_prsnt_actions),b_prsnt_actions), axis = 0)\n",
    "'''\n",
    "############################################################\n",
    "state_dim = 2*N*J\n",
    "action_dim = N*J\n",
    "max_action = 1.0\n",
    "time_slots = np.arange(0,Ts)\n",
    "\n",
    "agent = DDPG(state_dim, action_dim, max_action)\n",
    "replay_buffer = ReplayBuffer()\n",
    "\n",
    "#train_agents(env, agent1, agent2, replay_buffer)\n",
    "episodes=500\n",
    "batch_size=128\n",
    "\n",
    "ts_counter = 0\n",
    "\n",
    "######## generate interactions dummy ###########\n",
    "interfers_actions = np.zeros((M-1, J, N)) # should be the actions from other agent. \n",
    "for i, m in enumerate(range(M-1)):\n",
    "    interfers_actions[i,:,:] = np.transpose(generate_matrix(N, J))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4174ff86",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 1, Reward: -3.1648969479359845\n",
      "Episode: 2, Reward: -8.341143492964834\n",
      "Episode: 3, Reward: 3.774591116231532\n",
      "Episode: 4, Reward: 9.745596307338428\n",
      "Episode: 5, Reward: 5.761370130574362\n",
      "Episode: 6, Reward: 6.600996808682471\n",
      "Episode: 7, Reward: 15.323590877627902\n",
      "Episode: 8, Reward: 7.798492086525141\n",
      "Episode: 9, Reward: 7.223440928032269\n",
      "Episode: 10, Reward: 3.5485601380770837\n",
      "Episode: 11, Reward: 0.1379508081964343\n",
      "Episode: 12, Reward: 3.676322044198065\n",
      "Episode: 13, Reward: 3.802974673325786\n",
      "Episode: 14, Reward: 3.7604507577001485\n",
      "Episode: 15, Reward: 4.424157847977862\n",
      "Episode: 16, Reward: 8.662867341954806\n",
      "Episode: 17, Reward: 3.762151407994483\n",
      "Episode: 18, Reward: 1.5713122742784733\n",
      "Episode: 19, Reward: 12.988238512347234\n",
      "Episode: 20, Reward: 17.3829729747958\n",
      "Episode: 21, Reward: 8.190686122287303\n",
      "Episode: 22, Reward: 11.955823665139745\n",
      "Episode: 23, Reward: 8.074108618721484\n",
      "Episode: 24, Reward: 11.40330869820259\n",
      "Episode: 25, Reward: 8.337899094804584\n",
      "Episode: 26, Reward: 14.266902278331141\n",
      "Episode: 27, Reward: 13.221603570621605\n",
      "Episode: 28, Reward: 10.488237689372227\n",
      "Episode: 29, Reward: 6.602649412514492\n",
      "Episode: 30, Reward: 9.7213898714634\n",
      "Episode: 31, Reward: 11.267226483793591\n",
      "Episode: 32, Reward: 14.323506705400494\n",
      "Episode: 33, Reward: 8.345486835981552\n",
      "Episode: 34, Reward: 10.07593806536215\n",
      "Episode: 35, Reward: 14.876798118589054\n",
      "Episode: 36, Reward: 10.21473255244575\n",
      "Episode: 37, Reward: 14.047428942997662\n",
      "Episode: 38, Reward: 12.77375009106415\n",
      "Episode: 39, Reward: 14.412757468834219\n",
      "Episode: 40, Reward: 14.907624722299035\n",
      "Episode: 41, Reward: 14.59686260834159\n",
      "Episode: 42, Reward: 11.635491599330708\n",
      "Episode: 43, Reward: 18.96176966716442\n",
      "Episode: 44, Reward: 16.464362161687003\n",
      "Episode: 45, Reward: 14.32498432826954\n",
      "Episode: 46, Reward: 15.88332519727226\n",
      "Episode: 47, Reward: 16.99668681985771\n",
      "Episode: 48, Reward: 14.368739332417123\n",
      "Episode: 49, Reward: 10.191786119705077\n",
      "Episode: 50, Reward: 7.458780978701806\n",
      "Episode: 51, Reward: 11.113679050753053\n",
      "Episode: 52, Reward: 9.427794263308847\n",
      "Episode: 53, Reward: 8.94668548379287\n",
      "Episode: 54, Reward: 10.40305563949452\n",
      "Episode: 55, Reward: 16.79071260117895\n",
      "Episode: 56, Reward: 9.547087385249627\n",
      "Episode: 57, Reward: 4.654907829343208\n",
      "Episode: 58, Reward: 11.995472344817815\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 22\u001b[0m\n\u001b[0;32m     19\u001b[0m action \u001b[38;5;241m=\u001b[39m agent\u001b[38;5;241m.\u001b[39mselect_action(np\u001b[38;5;241m.\u001b[39marray(state))\n\u001b[0;32m     20\u001b[0m \u001b[38;5;66;03m#action =  (action >= 0.5).astype(float)\u001b[39;00m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;66;03m#next_state, reward, done, _ = env.step(action)\u001b[39;00m\n\u001b[1;32m---> 22\u001b[0m nxt_state_lstm \u001b[38;5;241m=\u001b[39m get_lstm_pred_SINR(ts_start \u001b[38;5;241m=\u001b[39m ts_counter\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m, ts_end \u001b[38;5;241m=\u001b[39m ts_counter \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m11\u001b[39m)\n\u001b[0;32m     23\u001b[0m \u001b[38;5;66;03m#next_state = np.stack((np.multiply(nxt_state_lstm, action.reshape(N,J)),action.reshape(N,J)), axis = 0)\u001b[39;00m\n\u001b[0;32m     24\u001b[0m next_state \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mstack((nxt_state_lstm,action\u001b[38;5;241m.\u001b[39mreshape(N,J)), axis \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m)\n",
      "Cell \u001b[1;32mIn[4], line 7\u001b[0m, in \u001b[0;36mget_lstm_pred_SINR\u001b[1;34m(ts_start, ts_end)\u001b[0m\n\u001b[0;32m      5\u001b[0m inp \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m ts \u001b[38;5;129;01min\u001b[39;00m np\u001b[38;5;241m.\u001b[39marange(ts_start, ts_end):\n\u001b[1;32m----> 7\u001b[0m     SINR \u001b[38;5;241m=\u001b[39m  environ\u001b[38;5;241m.\u001b[39mget_next_state(alltime_PathGains, alltime_fast_fading_gains, ts \u001b[38;5;241m=\u001b[39m ts)[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m      8\u001b[0m     inp\u001b[38;5;241m.\u001b[39mappend(SINR[:,j])\n\u001b[0;32m      9\u001b[0m inp \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(inp)\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m1\u001b[39m,history,N)\n",
      "File \u001b[1;32m~\\OneDrive - Aalto University\\Simulations\\RL framework URLLC\\In-Xsubnetworks_URLLC_asynchronous_update\\env.py:359\u001b[0m, in \u001b[0;36menv.get_next_state\u001b[1;34m(self, alltime_PathGains, alltime_fast_fading_gains, ts, b, interfers_actions, b_actions)\u001b[0m\n\u001b[0;32m    357\u001b[0m         b_actions \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate_matrix(N, J)\n\u001b[0;32m    358\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m i, m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mrange\u001b[39m(M\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)):\n\u001b[1;32m--> 359\u001b[0m             interfers_actions[i,:,:] \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate_matrix(N, J))\n\u001b[0;32m    361\u001b[0m \u001b[38;5;66;03m###### the below code is the samething as in compute rewards method #########\u001b[39;00m\n\u001b[0;32m    362\u001b[0m PathGainsTot \u001b[38;5;241m=\u001b[39m alltime_PathGains[ts,:,:,:]\u001b[38;5;241m*\u001b[39malltime_fast_fading_gains[ts,:,:,:]\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "epi_rewards = []\n",
    "b_prev_actions =  generate_matrix(N, J)\n",
    "P_e = []\n",
    "for episode in range(episodes):\n",
    "        ############# get initial states  #############\n",
    "        if episode == 0:\n",
    "            state_lstm = get_lstm_pred_SINR(ts_start = 0, ts_end = 10)\n",
    "        else:\n",
    "            state_lstm = get_lstm_pred_SINR(ts_start = ts_counter, ts_end = ts_counter+10)\n",
    "        #state = np.stack((np.multiply(state_lstm, b_prev_actions),b_prev_actions), axis = 0)\n",
    "        state = np.stack((state_lstm,b_prev_actions), axis = 0)\n",
    "        #b_prsnt_actions =  generate_matrix(N, J)\n",
    "        #nxt_state_lstm = get_lstm_pred_SINR(ts_start = 1, ts_end = 11)\n",
    "        #next_state = np.stack((np.multiply(nxt_state_lstm, b_prsnt_actions),b_prsnt_actions), axis = 0)\n",
    "        ############################################################\n",
    "        episode_reward = 0\n",
    "\n",
    "        for t in range(T):\n",
    "            action = agent.select_action(np.array(state))\n",
    "            #action =  (action >= 0.5).astype(float)\n",
    "            #next_state, reward, done, _ = env.step(action)\n",
    "            nxt_state_lstm = get_lstm_pred_SINR(ts_start = ts_counter+1, ts_end = ts_counter + 11)\n",
    "            #next_state = np.stack((np.multiply(nxt_state_lstm, action.reshape(N,J)),action.reshape(N,J)), axis = 0)\n",
    "            next_state = np.stack((nxt_state_lstm,action.reshape(N,J)), axis = 0)\n",
    "            \n",
    "            \n",
    "            state = next_state\n",
    "            #reward = np.sum(environ.compute_rewards(alltime_PathGains, alltime_fast_fading_gains, ts = time_slots[ts_counter + 10],\n",
    "                                                    #b = 0, interfers_actions = interfers_actions, b_actions = b_prev_actions)[2])\n",
    "            _, p_e, reward = environ.compute_rewards(alltime_PathGains, alltime_fast_fading_gains, ts = time_slots[ts_counter + 10],\n",
    "                                                    b = 0, interfers_actions = interfers_actions, b_actions = b_prev_actions)\n",
    "            P_e.append(p_e)\n",
    "            reward = np.sum(reward)\n",
    "            #print(t, p_e, reward)\n",
    "            episode_reward += reward\n",
    "            replay_buffer.add(state, next_state, action.reshape(N,J), reward)\n",
    "\n",
    "            if replay_buffer.size() > batch_size:\n",
    "                agent.train(replay_buffer, t, batch_size)\n",
    "            ts_counter+=1\n",
    "            if ts_counter == Ts - 10:\n",
    "                ts_counter = 0\n",
    "            #print(ts_counter)\n",
    "        b_prev_actions = action.reshape(N,J)\n",
    "        print(f\"Episode: {episode + 1}, Reward: {episode_reward/256}\")\n",
    "        epi_rewards.append(episode_reward/256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4767ebfc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\sriniva3\\\\OneDrive - Aalto University\\\\Simulations\\\\RL framework URLLC\\\\In-Xsubnetworks_URLLC_asynchronous_update'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f767c14",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(epi_rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "301a6cce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame({'values':epi_rewards})\n",
    "df['rolling_mean'] = df['values'].rolling(window=5).mean()\n",
    "plt.plot(df['rolling_mean'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8e339954",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\sriniva3\\\\OneDrive - Aalto University\\\\Simulations\\\\RL framework URLLC\\\\In-Xsubnetworks_URLLC_asynchronous_update'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "041a48c0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95dc2d03",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aca2d004",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8fa3ace",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d526b2e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3234cba2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c8eeab1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "T = 256\n",
    "############# get initial states  #############\n",
    "b_prev_actions =  generate_matrix(N, J)\n",
    "state_lstm = get_lstm_pred_SINR(ts_start = 0, ts_end = 10)\n",
    "state = np.stack((np.multiply(state_lstm, b_prev_actions),b_prev_actions), axis = 0)\n",
    "b_prsnt_actions =  generate_matrix(N, J)\n",
    "nxt_state_lstm = get_lstm_pred_SINR(ts_start = 1, ts_end = 11)\n",
    "next_state = np.stack((np.multiply(nxt_state_lstm, b_prsnt_actions),b_prsnt_actions), axis = 0)\n",
    "############################################################\n",
    "state_dim = 2*N*J\n",
    "action_dim = N*J\n",
    "max_action = 1.0\n",
    "time_slots = np.arange(0,Ts)\n",
    "\n",
    "agent = DDPG(state_dim, action_dim, max_action)\n",
    "replay_buffer = ReplayBuffer()\n",
    "\n",
    "#train_agents(env, agent1, agent2, replay_buffer)\n",
    "episodes=500\n",
    "batch_size=64\n",
    "ts_counter = 2\n",
    "for episode in range(episodes):  \n",
    "        #state1 = env.reset()  # Reset the environment for agent1\n",
    "\n",
    "        \n",
    "        episode_reward = 0\n",
    "\n",
    "        counter = 0\n",
    "        for t in range(T):\n",
    "            action = agent.select_action(np.array(state))\n",
    "            #pdb.set_trace()\n",
    "            #next_state1, reward1, done1, _, _ = env.step(action1)\n",
    "\n",
    "            #b_prsnt_actions =  generate_matrix(N, J)\n",
    "            b_prsnt_actions = action.reshape((N,J))\n",
    "            nxt_state_lstm = get_lstm_pred_SINR(ts_start = ts_counter, ts_end = ts_counter+10)\n",
    "            next_state = np.stack((np.multiply(nxt_state_lstm, b_prsnt_actions),b_prsnt_actions), axis = 0)\n",
    "\n",
    "            state = next_state\n",
    "            ######## compute reward for one agent ###########\n",
    "            interfers_actions = np.zeros((M-1, J, N)) # should be the actions from other agent. \n",
    "            for i, m in enumerate(range(M-1)):\n",
    "                        interfers_actions[i,:,:] = np.transpose(generate_matrix(N, J))\n",
    "\n",
    "            reward = np.sum(environ.compute_rewards(alltime_PathGains, alltime_fast_fading_gains, ts = time_slots[ts_counter],\n",
    "                                                    b = 0, interfers_actions = interfers_actions, b_actions = b_prev_actions)[2])\n",
    "\n",
    "            episode_reward += reward\n",
    "            replay_buffer.add(state, next_state, b_prsnt_actions, reward)\n",
    "            b_prev_actions = b_prsnt_actions\n",
    "                    \n",
    "\n",
    "            # Train both agents independently if enough samples are in the buffer\n",
    "            if len(replay_buffer.storage) > batch_size:\n",
    "                        state, next_state, action, reward = replay_buffer.sample(batch_size)\n",
    "\n",
    "                        state = torch.FloatTensor(state).to(torch.device(\"cpu\"))\n",
    "                        next_state = torch.FloatTensor(next_state).to(torch.device(\"cpu\"))\n",
    "                        action = torch.FloatTensor(action).to(torch.device(\"cpu\"))\n",
    "                        reward = torch.FloatTensor(reward).to(torch.device(\"cpu\"))\n",
    "                        if t < 256:\n",
    "                            not_done = 1.0\n",
    "                            #not_done = torch.FloatTensor(not_done).to(torch.device(\"cpu\"))\n",
    "                        else:\n",
    "                            not_done = 0.0\n",
    "                            #not_done = torch.FloatTensor(not_done).to(torch.device(\"cpu\"))\n",
    "\n",
    "                        # Select action according to policy and add clipped noise\n",
    "                        next_action = agent.actor_target(next_state.reshape(batch_size, 1, -1))\n",
    "                        noise = (torch.randn_like(next_action) * 0.2).clamp(-0.5, 0.5)\n",
    "                        next_action = (next_action + noise).clamp(-agent.max_action, agent.max_action)\n",
    "\n",
    "                        # Compute the target Q value\n",
    "                        target_q1, target_q2 = agent.critic_target(next_state.reshape(batch_size, 1, -1).squeeze(1), \n",
    "                                                                   next_action.squeeze(1))\n",
    "                        target_q = torch.min(target_q1, target_q2)\n",
    "                        target_q = reward + not_done * agent.discount * target_q.detach()\n",
    "\n",
    "                        # Get current Q estimates\n",
    "                        current_q1, current_q2 = agent.critic(state.reshape(batch_size, 1, -1).squeeze(1), \n",
    "                                            action.reshape(batch_size, 1, -1).squeeze(1))\n",
    "                        # Compute critic loss\n",
    "                        critic_loss = F.mse_loss(current_q1, target_q) + F.mse_loss(current_q2, target_q)\n",
    "\n",
    "                        # Optimize the critic\n",
    "                        agent.critic_optimizer.zero_grad()\n",
    "                        critic_loss.backward()\n",
    "                        agent.critic_optimizer.step()\n",
    "\n",
    "                        # Compute actor loss\n",
    "                        actor_loss = -agent.critic(state.reshape(batch_size, 1, -1).squeeze(1), \n",
    "              agent.actor(state.reshape(batch_size, 1, -1).squeeze(1)))[0].mean()\n",
    "\n",
    "                        # Optimize the actor\n",
    "                        agent.actor_optimizer.zero_grad()\n",
    "                        actor_loss.backward()\n",
    "                        agent.actor_optimizer.step()\n",
    "\n",
    "                        # Update the frozen target models\n",
    "                        for param, target_param in zip(agent.critic.parameters(), agent.critic_target.parameters()):\n",
    "                            target_param.data.copy_(agent.tau * param.data + (1 - agent.tau) * target_param.data)\n",
    "\n",
    "                        for param, target_param in zip(agent.actor.parameters(), agent.actor_target.parameters()):\n",
    "                            target_param.data.copy_(agent.tau * param.data + (1 - agent.tau) * target_param.data)\n",
    "\n",
    "            print(counter, reward)\n",
    "            counter+=1\n",
    "            ts_counter+=1\n",
    "            if ts_counter == Ts-1:\n",
    "                ts_counter = 2\n",
    "        print(f\"Episode: {episode}, Agent 1 Reward: {episode_reward/256}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50d5533e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "action = \n",
    "action.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeced498",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d23bf216",
   "metadata": {},
   "outputs": [],
   "source": [
    "state.reshape((batch_size,1,-1)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d7f778c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "agent.select_action(state.reshape((batch_size,1,-1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72cd24af",
   "metadata": {},
   "outputs": [],
   "source": [
    "state = state.reshape((batch_size,1,-1))\n",
    "state.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0edc0fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "state.reshape(1, -1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e5feac6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
